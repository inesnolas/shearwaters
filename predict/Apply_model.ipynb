{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) define list of wav files, load model\n",
    "\n",
    "2) for each wav file\n",
    "\n",
    "    load first segment,\n",
    "    \n",
    "        get spectrogram\n",
    "    \n",
    "        preprocess, normalize scale...\n",
    "\n",
    "        ([option] save data! or jump previous step because it has already been saved?)\n",
    "        \n",
    "        aply model\n",
    "        \n",
    "        get predictions\n",
    "        \n",
    "        get first segment annotations\n",
    "    \n",
    "    load 2nd segment\n",
    "    \n",
    "        get spectrogram\n",
    "    \n",
    "        preprocess, normalize scale...\n",
    "\n",
    "        ([option] save data! or jump previous step because it has already been saved?)\n",
    "    \n",
    "        apply model\n",
    "        \n",
    "        get predictions\n",
    "        \n",
    "        concatenate new annotation to previous anotation file\n",
    "     \n",
    "     etc.. till end of wav file\n",
    "         \n",
    "     save single anotation file for whole wav.\n",
    "    \n",
    "    \n",
    "3) for each prediction annotation data that becomes available, \n",
    "\n",
    "    run code to compute regions files for sonic visualizer!\n",
    "        \n",
    "        transform list of prediction matrix per segment [audiox_startSec_to_finishSec, pred_matrix]\n",
    "        \n",
    "    this can be done in separate/ parallel to the first step...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import pickle\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers import Reshape, Permute, multiply\n",
    "from keras.layers import TimeDistributed, Dense, Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras import backend as K\n",
    "import datetime\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(x): # modified for single spectrogram. n =1, no labels!\n",
    "    \"\"\"Reformat data into a suitable format.\n",
    "    \n",
    "    # Arguments\n",
    "        dataset: dataset in format (id, spectro, label)\n",
    "        \n",
    "    # Returns\n",
    "        x: spectros normalised across each mel band in format (n, timesteps, mel bands, 1)\n",
    "        y: labels in format (n, timesteps, 8)\n",
    "    \"\"\"\n",
    "#     x = dataset[:, 1] \n",
    "#     x = np.stack(x) # reshape to (n, mel bands, timesteps)\n",
    "    x = np.expand_dims(np.moveaxis(x, 1, -1), axis=3) # reformat x to (n, timesteps, mel bands, 1)  \n",
    "#     y = dataset[:, 2] \n",
    "#     y = np.moveaxis(np.stack(y), 1, -1) # reformat y to (n, timesteps, 8)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 6\n",
    "slide = 6\n",
    "mels = 64\n",
    "sample_rate = 22_050\n",
    "timesteps = 259 # number of frames in a segment! =((SR* segment_size)/ hop_length)\n",
    "timesteps_per_second = timesteps / segment_size\n",
    "\n",
    "############# settings: ##################################################################\n",
    "save_path =  #where to save the prediction matrix to\n",
    "wavs_list = [] # list of wav files to make annotations\n",
    "model_path =   # which model to use (path)\n",
    "\n",
    "prediction_theshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load model:\n",
    "\n",
    "model = load_model(model_path)\n",
    "#\n",
    "\n",
    "\n",
    "for wfile in wavs_list:\n",
    "    prediction_annotations = []\n",
    "    \n",
    "    print('load audio')\n",
    "    #Load audio,\n",
    "    y, sr = librosa.load(wfile, sr=sample_rate)\n",
    "    length = int(len(y) / sr) #seconds\n",
    "    remainder = length % segment_size\n",
    "    #     import pdb; pdb.set_trace()\n",
    "\n",
    "    audio_id = wfile[:-4]\n",
    "    \n",
    "#   segment, cycle for each segment:\n",
    "    for t in tqdm(range(0, length - remainder - segment_size, slide),\n",
    "                  desc='create_spectros'):\n",
    "        start = t\n",
    "        stop = t + segment_size\n",
    "        print('start in frames', str(sr*start))\n",
    "        print('stop in frames: ', str((sr*stop)))\n",
    "        current_y = y[sr*start:(sr*stop)]\n",
    "        \n",
    "        # create spectrogram\n",
    "        spectro = librosa.feature.melspectrogram(y=current_y, sr=sr, n_mels=mels,\n",
    "                                                 fmax=sr/2)\n",
    "        dt_id = audio_id + '_' + str(start) + 's_to' + str(stop) + 's'\n",
    "        print(dt_id)\n",
    "        \n",
    "#       treat segment spectrogram for network\n",
    "#          PCEN:\n",
    "        spectro_PCEN = librosa.pcen(spectro * (2**31))\n",
    "#         further normalize, scale?\n",
    "        x = reformat(spectro_PCEN)\n",
    "#          downsample mels???? uhh, didnt realize we were doing this... mels = 45\n",
    "        x = x[:mels]\n",
    "        \n",
    "        \n",
    "        \n",
    "#          predict with loaded model\n",
    "        predicted = model.predict(x) \n",
    "#          get binary predictions accordingly to threshold\n",
    "        predicted[predicted > prediction_theshold] = 1\n",
    "        predicted[predicted <= prediction_theshold] = 0\n",
    "        \n",
    "        prediction_annotations.append([dt_id, predicted])\n",
    "        \n",
    "    #save data processed for this file\n",
    "    d_array = np.asarray(prediction_annotations)\n",
    "    np.save(save_path+audio_id, d_array)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
